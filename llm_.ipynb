{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b120826a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/richardmiller/Documents/GitHub/llm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from itertools import chain\n",
    "\n",
    "fpath_win = 'C:\\\\Users\\\\rwmil\\\\Documents\\\\GitHub\\\\llm\\\\llm\\\\'\n",
    "fpath_mac = '/Users/richardmiller/Documents/GitHub/llm/'\n",
    "os.chdir(fpath_mac)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ded437",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./LLMs-from-scratch-main/ch02/01_main-chapter-code/the-verdict.txt','r',encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "f.close()\n",
    "\n",
    "split_text = [re.split(r'([,.:;?_!\"()\\']|--|\\s)',i) for i in raw_text.split()]\n",
    "split_text = [i.strip() for item in split_text for i in item if i not in ['',' ']]\n",
    "split_text.extend(['<|endoftext|>','<|unk|>'])\n",
    "\n",
    "split_text = sorted(list(set(split_text)))\n",
    "vocab = {token:i for i,token in enumerate(split_text)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f22941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self,vocab):\n",
    "        ''' The vocab object must be a dictionary or \n",
    "        dictionary-like object and must be defined \n",
    "        outside of the tokenizer class. It also must\n",
    "        have the form of {string:integer}'''\n",
    "        self.string_to_int = vocab\n",
    "        self.int_to_string = {i:s for s,i in vocab.items()}\n",
    "        self.re_string_encoder = r'([,.:;?_!\"()\\']|--|\\s)' #Leave these here because I can use them later to define the regex\n",
    "        self.re_string_decoder = r'\\s+([,.:;?!\"()\\'])' # strings that I want to use later. If my texts become more complex.\n",
    "\n",
    "    def encoder(self,text):\n",
    "        preprocess = re.split(self.re_string_encoder,text)\n",
    "        preprocess = [i.strip() for i in preprocess if i not in ['',' ']]\n",
    "        preprocess = [i if i in self.string_to_int else '<|unk|>' for i in preprocess]\n",
    "        \n",
    "        \n",
    "        return [self.string_to_int[s] for s in preprocess]\n",
    "\n",
    "    def decoder(self,ids):\n",
    "        text = ' '.join([self.int_to_string[i] for i in ids])\n",
    "        text = re.sub(self.re_string_decoder,r'\\1',text)\n",
    "        return text\n",
    "\n",
    "tokenizer = SimpleTokenizer(vocab)\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "te = \" <|endoftext|> \".join((text1, text2))\n",
    "out = tokenizer.encoder(te)\n",
    "out2 = tokenizer.decoder(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c15ca2-2e84-4919-82bd-3d97caec0a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a2d9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccff1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "text = ('Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.')\n",
    "integers = tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0347d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a50db57",
   "metadata": {},
   "source": [
    "## Section 2 Using Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "\n",
    "fpath_win = 'C:\\\\Users\\\\rwmil\\\\Documents\\\\GitHub\\\\llm\\\\llm\\\\'\n",
    "fpath_mac = '/Users/richardmiller/Documents/GitHub/llm/'\n",
    "os.chdir(fpath_mac)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a7706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get text data\n",
    "with open('./LLMs-from-scratch-main/ch02/01_main-chapter-code/the-verdict.txt') as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "f.close()\n",
    "\n",
    "#Initialize tokenizer and encode raw text from file\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "encoded_text = tokenizer.encode(raw_text)\n",
    "\n",
    "#Remove last 50 characters\n",
    "encoded_sample = encoded_text[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473e23f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split encoded sample into x and y targets\n",
    "\n",
    "context_size = 4 #Determines number of tokens included in input\n",
    "x = encoded_sample[:context_size]\n",
    "y = encoded_sample[1:context_size+1]\n",
    "\n",
    "for i in range(1,context_size+1):\n",
    "    context = encoded_sample[:i]\n",
    "    desired = encoded_sample[i]\n",
    "    print(context,'----->',desired)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1049d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,context_size+1):\n",
    "    context = encoded_sample[:i]\n",
    "    desired = encoded_sample[i]\n",
    "    print(tokenizer.decode(context),'----->',tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d52d9",
   "metadata": {},
   "source": [
    "## Section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2aa17e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "818fa946",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self,txt,tokenizer,max_length,stride):\n",
    "        '''\n",
    "        Looks like tokenizer is instantiated before being passed to this class.\n",
    "        '''\n",
    "        \n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        #Get token id's\n",
    "        self.token_ids = tokenizer.encode(txt)\n",
    "        \n",
    "        for i in range(0,len(self.token_ids)-max_length,stride):\n",
    "            input_chunk = self.token_ids[i:i+max_length]\n",
    "            target_chunk = self.token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ids, self.target_ids\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df9798f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(\n",
    "    txt,\n",
    "    batch_size=25,\n",
    "    max_length=255,\n",
    "    stride=128,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "):\n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    dataset = GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    return dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a6c4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./LLMs-from-scratch-main/ch02/01_main-chapter-code/the-verdict.txt') as f:\n",
    "    raw_data = f.read()\n",
    "    \n",
    "f.close()\n",
    "\n",
    "#max_length and stride can be changed, using a larger stride will\n",
    "#result in less overfitting\n",
    "\n",
    "#batch_size is a tunable hyperparameter to make the model better.\n",
    "#Higher batch_size means more memory usage and less noise\n",
    "#Lower batch_size means less memory usage but more nosie in training\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_data,\n",
    "    batch_size=1,\n",
    "    max_length=4,\n",
    "    stride=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cac56403",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2,3,5,1])\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size,output_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bbf1b271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256]) torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size,output_dim)\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_data,batch_size=8,max_length=max_length,stride=max_length,shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs,targets = next(data_iter)\n",
    "\n",
    "token_embeddings = token_embedding_layer(inputs[1])\n",
    "\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length,output_dim)\n",
    "pos_embedding = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embedding.shape,token_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8b7c0b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = token_embeddings+pos_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9856ba2b",
   "metadata": {},
   "source": [
    "# Chapter 3 - Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "81899b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "inputs = torch.tensor([\n",
    "    [0.43,0.15,0.89],\n",
    "    [0.55,0.87,0.66],\n",
    "    [0.57,0.85,0.64],\n",
    "    [0.22,0.58,0.33],\n",
    "    [0.77,0.25,0.10],\n",
    "    [0.05,0.80,0.55],\n",
    "])\n",
    "\n",
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i,query)\n",
    "    \n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "adc244b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum:  tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2/attn_scores_2.sum()\n",
    "print('Attention Weights: ',attn_weights_2_tmp)\n",
    "print('Sum: ',attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b8ce5e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "#Computing attention weights of a single token by scalar multiplication of a vector\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "    \n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d38a9fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "#Computing attention weights of all tokens by dot product\n",
    "attn_scores = torch.empty(6,6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i,j] = torch.dot(x_i,x_j)\n",
    "        \n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e8808d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs@inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "acc8296f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores,dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26512567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
