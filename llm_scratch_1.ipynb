{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b120826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import chain\n",
    "\n",
    "fpath_win = 'C:\\\\Users\\\\rwmil\\\\Documents\\\\GitHub\\\\llm\\\\llm\\\\'\n",
    "fpath_mac = '/Users/richardmiller/Documents/GitHub/llm/'\n",
    "os.chdir(fpath_mac)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ded437",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./LLMs-from-scratch-main/ch02/01_main-chapter-code/the-verdict.txt','r',encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "f.close()\n",
    "\n",
    "split_text = [re.split(r'([,.:;?_!\"()\\']|--|\\s)',i) for i in raw_text.split()]\n",
    "split_text = [i.strip() for item in split_text for i in item if i not in ['',' ']]\n",
    "split_text.extend(['<|endoftext|>','<|unk|>'])\n",
    "\n",
    "split_text = sorted(list(set(split_text)))\n",
    "vocab = {token:i for i,token in enumerate(split_text)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f22941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self,vocab):\n",
    "        ''' The vocab object must be a dictionary or \n",
    "        dictionary-like object and must be defined \n",
    "        outside of the tokenizer class. It also must\n",
    "        have the form of {string:integer}'''\n",
    "        self.string_to_int = vocab\n",
    "        self.int_to_string = {i:s for s,i in vocab.items()}\n",
    "        self.re_string_encoder = r'([,.:;?_!\"()\\']|--|\\s)' #Leave these here because I can use them later to define the regex\n",
    "        self.re_string_decoder = r'\\s+([,.:;?!\"()\\'])' # strings that I want to use later. If my texts become more complex.\n",
    "\n",
    "    def encoder(self,text):\n",
    "        preprocess = re.split(self.re_string_encoder,text)\n",
    "        preprocess = [i.strip() for i in preprocess if i not in ['',' ']]\n",
    "        preprocess = [i if i in self.string_to_int else '<|unk|>' for i in preprocess]\n",
    "        \n",
    "        \n",
    "        return [self.string_to_int[s] for s in preprocess]\n",
    "\n",
    "    def decoder(self,ids):\n",
    "        text = ' '.join([self.int_to_string[i] for i in ids])\n",
    "        text = re.sub(self.re_string_decoder,r'\\1',text)\n",
    "        return text\n",
    "\n",
    "tokenizer = SimpleTokenizer(vocab)\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "te = \" <|endoftext|> \".join((text1, text2))\n",
    "out = tokenizer.encoder(te)\n",
    "out2 = tokenizer.decoder(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c15ca2-2e84-4919-82bd-3d97caec0a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0062887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df41960",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "text = ('Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.')\n",
    "integers = tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d483273",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bcba90",
   "metadata": {},
   "source": [
    "## Section 2 Using Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7057da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "\n",
    "fpath_win = 'C:\\\\Users\\\\rwmil\\\\Documents\\\\GitHub\\\\llm\\\\llm\\\\'\n",
    "fpath_mac = '/Users/richardmiller/Documents/GitHub/llm/'\n",
    "os.chdir(fpath_mac)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58110c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get text data\n",
    "with open('./LLMs-from-scratch-main/ch02/01_main-chapter-code/the-verdict.txt') as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "f.close()\n",
    "\n",
    "#Initialize tokenizer and encode raw text from file\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "encoded_text = tokenizer.encode(raw_text)\n",
    "\n",
    "#Remove last 50 characters\n",
    "encoded_sample = encoded_text[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d105e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split encoded sample into x and y targets\n",
    "\n",
    "context_size = 4 #Determines number of tokens included in input\n",
    "x = encoded_sample[:context_size]\n",
    "y = encoded_sample[1:context_size+1]\n",
    "\n",
    "for i in range(1,context_size+1):\n",
    "    context = encoded_sample[:i]\n",
    "    desired = encoded_sample[i]\n",
    "    print(context,'----->',desired)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e0e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,context_size+1):\n",
    "    context = encoded_sample[:i]\n",
    "    desired = encoded_sample[i]\n",
    "    print(tokenizer.decode(context),'----->',tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097ea36",
   "metadata": {},
   "source": [
    "## Section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ee2b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbbc7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self,txt,tokenizer,max_length,stride):\n",
    "        '''\n",
    "        Looks like tokenizer is instantiated before being passed to this class.\n",
    "        '''\n",
    "        \n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        #Get token id's\n",
    "        self.token_ids = tokenizer.encode(txt)\n",
    "        \n",
    "        for i in range(0,len(self.token_ids)-max_length,stride):\n",
    "            input_chunk = self.token_ids[i:i+max_length]\n",
    "            target_chunk = self.token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ids, self.target_ids\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642d41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(\n",
    "    txt,\n",
    "    batch_size=25,\n",
    "    max_length=255,\n",
    "    stride=128,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "):\n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    dataset = GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    return dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cec9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./LLMs-from-scratch-main/ch02/01_main-chapter-code/the-verdict.txt') as f:\n",
    "    raw_data = f.read()\n",
    "    \n",
    "f.close()\n",
    "\n",
    "#max_length and stride can be changed, using a larger stride will\n",
    "#result in less overfitting\n",
    "\n",
    "#batch_size is a tunable hyperparameter to make the model better.\n",
    "#Higher batch_size means more memory usage and less noise\n",
    "#Lower batch_size means less memory usage but more nosie in training\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_data,\n",
    "    batch_size=1,\n",
    "    max_length=4,\n",
    "    stride=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9184df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2,3,5,1])\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size,output_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac03de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size,output_dim)\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_data,batch_size=8,max_length=max_length,stride=max_length,shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs,targets = next(data_iter)\n",
    "\n",
    "token_embeddings = token_embedding_layer(inputs[1])\n",
    "\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length,output_dim)\n",
    "pos_embedding = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embedding.shape,token_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae94c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = token_embeddings+pos_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bc487e",
   "metadata": {},
   "source": [
    "# Chapter 3 - Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ed2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor([\n",
    "    [0.43,0.15,0.89],\n",
    "    [0.55,0.87,0.66],\n",
    "    [0.57,0.85,0.64],\n",
    "    [0.22,0.58,0.33],\n",
    "    [0.77,0.25,0.10],\n",
    "    [0.05,0.80,0.55],\n",
    "])\n",
    "\n",
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i,query)\n",
    "    \n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134f101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_2_tmp = attn_scores_2/attn_scores_2.sum()\n",
    "print('Attention Weights: ',attn_weights_2_tmp)\n",
    "print('Sum: ',attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2496cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing attention weights of a single token by scalar multiplication of a vector\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "    \n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6130248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing attention weights of all tokens by dot product\n",
    "attn_scores = torch.empty(6,6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i,j] = torch.dot(x_i,x_j)\n",
    "        \n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9cea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = inputs@inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61a7b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = torch.softmax(attn_scores,dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec78f98",
   "metadata": {},
   "source": [
    "## Chatper 3 Section 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c712bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor([\n",
    "    [0.43,0.15,0.89],\n",
    "    [0.55,0.87,0.66],\n",
    "    [0.57,0.85,0.64],\n",
    "    [0.22,0.58,0.33],\n",
    "    [0.77,0.25,0.10],\n",
    "    [0.05,0.80,0.55],\n",
    "])\n",
    "\n",
    "#Define input element, input embadding size, and output embedding size\n",
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "\n",
    "#Initialize Weight tensors\n",
    "#requires_grad is set to false to reduce output clutter, but during model training it needs to be true\n",
    "torch.manual_seed(123) #For repeatability\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad=False)\n",
    "\n",
    "\n",
    "query_2 = x_2@W_query\n",
    "key_2 = x_2@W_key\n",
    "value_2 = x_2@W_value\n",
    "\n",
    "#Weight Tensors are optimized during training and are used to make predictions\n",
    "#Attention Weights are tensors that give the amount of attention to different parts of\n",
    "# the input. Attention weights are like saying since this word directly preceeds the unknown I\n",
    "# will pay more attention to it than I will to another word that is 40 tokens away.\n",
    "\n",
    "#Get all keys and values:\n",
    "keys = inputs@W_key\n",
    "values = inputs@W_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26bd57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing an individual weight\n",
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print('Attention score for position 2: ',attn_score_22)\n",
    "\n",
    "#Compute all weights using tensor dot product\n",
    "attn_scores_2 = query_2@keys.T\n",
    "print('Attention Scores: ',attn_scores_2)\n",
    "\n",
    "#Notice that the second value in attn_scores_2 matches the value of attn_score_22\n",
    "\n",
    "#Calculate attention scores from attention weights\n",
    "#To help normalize we are dividing by the square root\n",
    "#I think this is going to be like the standard deviation where\n",
    "# you divide by sqrt(N) for a population.\n",
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2/d_k**0.5, dim=-1)\n",
    "print('Scaled Attention Weights: ',attn_weights_2)\n",
    "\n",
    "#Calculate context vector\n",
    "context_vect_2 = attn_weights_2 @ values\n",
    "print('Context Vector: ',context_vect_2)\n",
    "\n",
    "#Query refers to the current item (token, word, sentance, etc.)\n",
    "#Key is the indexing value of a token and is used to match the query\n",
    "#Value the actual content or input values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d2eda2",
   "metadata": {},
   "source": [
    "## Chapter 3 Section 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c3a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It's useful to organize all the code in Chapter 3 Section 4.1 into a single\n",
    "# succinct python class for use in the LLM architecture.\n",
    "\n",
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self,d_in,d_out):\n",
    "        '''\n",
    "        Initializes weight tensors for queries, values, and keys.\n",
    "        Transforms from input dimension, d_in, to output dimension, d_out.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in,d_out))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        Compute keys, queries, and values tensors by cross product with their respective weights.\n",
    "        Calculate attention scores.\n",
    "        Normalize attention scores with softmax to get attention weight tensor.\n",
    "        Create a context vector.\n",
    "        \n",
    "        '''\n",
    "        keys = x@self.W_key\n",
    "        queries = x@self.W_query\n",
    "        values = x@self.W_value\n",
    "        \n",
    "        attn_scores = queries@keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores/keys.shape[-1]**0.5, \n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        context_vect = attn_weights@values\n",
    "        return context_vect\n",
    "    \n",
    "    \n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in,d_out)\n",
    "print(sa_v1(inputs))\n",
    "\n",
    "#NOTE: using nn.Linear layers will perform the matrix multiplication and with optimized weight initializations.\n",
    "# This should result in quicker and more reliable training versus using randomized weights.\n",
    "\n",
    "#Will impliment this in SelfAttention_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe596c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries@keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores/keys.shape[-1]**0.5,\n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        context_vect = attn_weights@values\n",
    "        \n",
    "        return context_vect\n",
    "    \n",
    "    \n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in,d_out)\n",
    "\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efe3f10",
   "metadata": {},
   "source": [
    "# Chapter 3 Section 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23ea1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Casual Self-Attention (AKA Masked Attention): Prevents a model from learning words after the token word. \n",
    "#Because the model won't have access to information after some token. Restricts the model's ability to learn\n",
    "# from the previous data before the token.\n",
    "\n",
    "#Multi-Head Self-Attention: Splits the Self-Attention mechanism into multiple sub-units or 'heads', this\n",
    "# splits up the learning process. Each head learns from different parts of the input text or different contexts.\n",
    "# This allows or increases the model's ability to perform complex tasks.\n",
    "\n",
    "#Consider the case of data represented as a tensor, T. In 2-dimensions, the current token data is the diagonal\n",
    "# elements of the input array. So, the values above the diagonal are masked. In a multi-dimensional tensor, this\n",
    "# would correspond to the diagonal hyper-plane. All elements in the volume above the diagonal hyper-plane\n",
    "# will be masked. This prevents the model from accessing data after a given token and restricts it to data from\n",
    "# before the given token.\n",
    "\n",
    "#The unmasked weights are normalized to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ca2ae4",
   "metadata": {},
   "source": [
    "## Chapter 3 Section 5.1 - Applying a Casual Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a66122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class from Chapter 3 Section 4.2\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries@keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores/keys.shape[-1]**0.5,\n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        context_vect = attn_weights@values\n",
    "        \n",
    "        return context_vect\n",
    "    \n",
    "    \n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in,d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75930eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute attention weights with the SelfAttention_v2 Class from\n",
    "# Chapter 3 Section 4.2\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries@keys.T\n",
    "attn_weights = torch.softmax(\n",
    "    attn_scores/keys.shape[-1]**0.5,\n",
    "    dim=-1\n",
    ")\n",
    "\n",
    "#Print the tensor we want to mask:\n",
    "print('Attention Weights Tensor: ',attn_weights)\n",
    "\n",
    "#Use tril() function to create a masking tensor with 0's in the upper triangle\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "\n",
    "print('Simple Masking Tensor: ',mask_simple)\n",
    "\n",
    "#Normalize Masking Tensor\n",
    "row_sums = mask_simple.sum(dim=-1,keepdim=True)\n",
    "mask_simple_norm = mask_simple/row_sums\n",
    "print('Normalized Masking Tensor: ', mask_simple_norm)\n",
    "\n",
    "#Masking and renormalizing will prevent upper triangular data from the softmax from leaking into\n",
    "# the masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476702f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improving on the masking function\n",
    "\n",
    "#Softmax converts the data into a Poisson probability distribution. We can represent zeros in the upper\n",
    "# triangle matrix as e^-inf. This can be beneficial to us. If we replace the upper triangle of the tensor with -inf\n",
    "# and then take the softmax, it will become an upper triangle of masked zeros.\n",
    "\n",
    "#Define an improved masking tensor:\n",
    "mask = torch.triu(torch.ones(context_length,context_length),diagonal=1) #diagonal=1 starts at the line above the diagonal\n",
    "masked = attn_scores.masked_fill(mask.bool(),-torch.inf) #Uses the masked_fill method in pytorch to fill the upper triangle with -infinity\n",
    "\n",
    "print('Masking Tensor: ',masked)\n",
    "\n",
    "#Make Attention Weights tensor\n",
    "attn_weights = torch.softmax(masked/keys.shape[-1]**0.5, dim=1)\n",
    "print('Attention Weights: ',attn_weights)\n",
    "\n",
    "#Calculate Context Tensor:\n",
    "context_vect = attn_weights@values\n",
    "print('Context Tensor: ', context_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a391b6c0",
   "metadata": {},
   "source": [
    "## Chapter 3 Section 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c1ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropout can be used to help prevent overfitting to available data and increase general usefulness of\n",
    "# your model. It does this by ignoring hidden layer units.\n",
    "\n",
    "#Dropout is only used during training and is disabled for model predictions.\n",
    "\n",
    "#For transformer architectures, dropout is typically employed at two points in the training process.\n",
    "# 1. After calculating attention weights\n",
    "# 2. After applying attention weights to value vectors\n",
    "\n",
    "#Here we will use a dropout rate of 50%, but typically dropout rates are 10% to 20% (0.1 to 0.2)\n",
    "\n",
    "#Apply the dropout to a 6x6 tensor for illustration purposes:\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6,6)\n",
    "print(dropout(example))\n",
    "\n",
    "#To account for half of the values being dropped out, the other values are doubled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the dropout method to the attention weights:\n",
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d564251",
   "metadata": {},
   "source": [
    "## Chapter 3 Section 5.3 - Implementing a Compact Casual Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impliment a CasualAttention class by modifying the SelfAttention class. This will then be used\n",
    "# later as a template for the MultiHeadAttion class that we will code.\n",
    "import torch.nn as nn\n",
    "class CasualAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in,\n",
    "        d_out,\n",
    "        context_length,\n",
    "        dropout,\n",
    "        qkv_bias = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(\n",
    "                context_length,\n",
    "                context_length), \n",
    "                       diagonal=1\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        b,num_tokens,d_in = x.shape\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries@keys.transpose(1,2)\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens,:num_tokens],\n",
    "            -torch.inf\n",
    "        )#################################\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores/keys.shape[-1]**0.5,\n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vect = attn_weights@values\n",
    "        \n",
    "        return context_vect\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c706ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a batched set of identical data inputs:\n",
    "inputs = torch.tensor([\n",
    "    [0.43,0.15,0.89],\n",
    "    [0.55,0.87,0.66],\n",
    "    [0.57,0.85,0.64],\n",
    "    [0.22,0.58,0.33],\n",
    "    [0.77,0.25,0.10],\n",
    "    [0.05,0.80,0.55],\n",
    "])\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "batch = torch.stack((inputs,inputs,), dim=0)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "\n",
    "\n",
    "ca = CasualAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vect = ca(batch)\n",
    "print('Context Tensor: ',context_vect.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eaf62f",
   "metadata": {},
   "source": [
    "## Chapter 3 Section 6 - Extending Single-Head Attention to Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1f7f47",
   "metadata": {},
   "source": [
    "## Chapter 3 Section 6.1 - Stacking Multiple Single-Head Attention Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab13c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                CasualAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    \n",
    "#By using list comprehension in the self.heads assignment, we are doing these in sequence. This is not\n",
    "# very efficient. But we can modify this class to perform assignments in parallel. This class has very\n",
    "# compact code, but it isn't very efficient. If we combine the MultiHeadAttentionWrapper class with the\n",
    "# CasualAttentionClass we can make it even more betterer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e867949",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([\n",
    "    [0.43,0.15,0.89],\n",
    "    [0.55,0.87,0.66],\n",
    "    [0.57,0.85,0.64],\n",
    "    [0.22,0.58,0.33],\n",
    "    [0.77,0.25,0.10],\n",
    "    [0.05,0.80,0.55],\n",
    "])\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "batch = torch.stack((inputs,inputs,), dim=0)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] #Number of tokens\n",
    "d_in,d_out = 3,1\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4c14c",
   "metadata": {},
   "source": [
    "## Chapter 3 Section 6.2 - Implimenting Multi-Head Attention with Weight Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e379a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_in, d_out,\n",
    "        context_length, dropout,\n",
    "        num_heads, qkv_bias=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert (d_out%num_heads==0), \"d_out must be divisible by num_heads with no remainder.\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(\n",
    "                torch.ones(\n",
    "                        context_length,\n",
    "                        context_length\n",
    "                ),\n",
    "                diagonal = 1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #Initialize values and weight tensors\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        #Reshape and resize weight tensors to inflate them (opposite of flattning)\n",
    "        # Tensors start with shape (b,num_tokens, d_out) and end with a shape of\n",
    "        # (b, num_tokens, num_heads, head_dim)\n",
    "        # where head_dim = d_out/num_heads\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "\n",
    "        #Perform matrix multiplication to get attention scores:\n",
    "        # This is analagous to having a tensor, T with shape (1,2,3,4) and performing 2 dot products:\n",
    "        #  first = T[0,0,:,:]\n",
    "        #  first_result = first@first.T\n",
    "        #\n",
    "        # and then performing:\n",
    "        #\n",
    "        #  second = T[0,1,:,:]\n",
    "        #. second_result = second@second.T\n",
    "        attn_scores = queries@keys.transpose(2,3)\n",
    "\n",
    "        #Mask attention scores\n",
    "        mask_bool = self.mask.bool()[:num_tokens,:num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        #Normalize attention scores\n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        #Dotproduct to get result\n",
    "        context_vec = (attn_weights@values).transpose(1,2)\n",
    "        \n",
    "        #Reshape outputs back to size (b,num_tokens,d_out)\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "            b, num_tokens, self.d_out\n",
    "        )\n",
    "\n",
    "        #Projection Layer - Not necessary, but often used in LLM architecture. I'm not sure why it's used.\n",
    "        # this layer projects the results from one vector space into another vector space, by some linear mapping.\n",
    "        # I'm not sure if this is to save disk space/memory, or if there is some optimal dimension for the vector\n",
    "        # space to decode this back to text. Maybe to decrease processing speed/cycles used.\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([\n",
    "    [0.43,0.15,0.89],\n",
    "    [0.55,0.87,0.66],\n",
    "    [0.57,0.85,0.64],\n",
    "    [0.22,0.58,0.33],\n",
    "    [0.77,0.25,0.10],\n",
    "    [0.05,0.80,0.55],\n",
    "])\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "batch = torch.stack((inputs,inputs,), dim=0)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_in,d_out,context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0309dc38",
   "metadata": {},
   "source": [
    "# Chapter 4 - Implimenting a GPT Model from Scratch to Generate Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b08423",
   "metadata": {},
   "source": [
    "## Chapter 4 Section 1 - Conding an LLM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT Model Configuration Variables\n",
    "GPT_CONFIG_124M = {\n",
    "            'vocab_size':50257,\n",
    "            'context_length':1024,\n",
    "            'emb_dim':768,\n",
    "            'n_heads':12,\n",
    "            'n_layers':12,\n",
    "            'drop_rate':0.1,\n",
    "            'qkv_bias':False,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11a8d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 4.1 - A Placeholder GPT Model Architecture Class\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "        #Placeholder for transformer blocks\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[\n",
    "                DummyTransformerBlock(cfg)\n",
    "                for _ in range(cfg['n_layers'])\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        #Placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg['emb_dim'],cfg['vocab_size'],bias=False\n",
    "        )\n",
    "        \n",
    "    def forward(self,in_idx):\n",
    "        print(in_idx.shape)\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "#Use pass through code placeholders for transformer and\n",
    "#Layer norm blocks.\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af088338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "\n",
    "def encoder(txt):\n",
    "    tensor = torch.tensor(tokenizer.encode(txt))\n",
    "    return tensor\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "txts = ['Every effort moves you','Every day holds a']\n",
    "batch = [encoder(tx) for tx in txts]\n",
    "batch = torch.stack(batch,dim=0)\n",
    "\n",
    "print(batch)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4768ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5,6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666732f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(mean, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd192697",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_norm = (out-mean)/torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(out_norm)\n",
    "print(mean, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed35f9",
   "metadata": {},
   "source": [
    "## Chapter 4 Section 2 - Normalizing Activations with Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a91ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 4.2 - A Layer Normalization Class\n",
    "#Code is implimented below the hashed line.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "        #Placeholder for transformer blocks\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[\n",
    "                DummyTransformerBlock(cfg)\n",
    "                for _ in range(cfg['n_layers'])\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        #Placeholder for LayerNorm\n",
    "        self.final_norm = LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg['emb_dim'],cfg['vocab_size'],bias=False\n",
    "        )\n",
    "        \n",
    "    def forward(self,in_idx):\n",
    "        print(in_idx.shape)\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "#Use pass through code placeholders for Transformer Block.\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "###############################################################################\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1,keepdim=True)\n",
    "        norm_x = (x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.scale*norm_x+self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1453d138",
   "metadata": {},
   "source": [
    "## Chapter 4 Section 3 - Implementing a Feed Forward Network with GELU Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60900e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 4.3 - An Implementation of the GELU Activation Function\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 0.5*x*(1+torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0/torch.pi))*\n",
    "            (x+0.044715*torch.pow(x,3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca798f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "gelu,relu = GELU(), nn.ReLU()\n",
    "x = torch.linspace(-3,3,100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8,3))\n",
    "\n",
    "for i, (y,label) in enumerate(zip([y_gelu,y_relu],['GELU','ReLU']), 1):\n",
    "    plt.subplot(1,2,i)\n",
    "    plt.plot(x,y)\n",
    "    plt.title(f'{label} activation function')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel(f'{label}(x)')\n",
    "    plt.grid(True)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81484321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 4.4 - A Feed Forward Neural Network Module\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg['emb_dim'], cfg['emb_dim']),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d325865",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2,3,768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29ae6d5",
   "metadata": {},
   "source": [
    "## Chapter 4 Section 4 - Adding Shortcut Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 4.5 - A Neural Network to Illustrate Shortcut Connections\n",
    "\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]),\n",
    "                GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]),\n",
    "                GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]),\n",
    "                GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]),\n",
    "                GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]),\n",
    "                GELU())\n",
    "            \n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n",
    "    \n",
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a function that prints the loss.\n",
    "def print_gradients(model,x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    loss.backward()\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f'{name} has a gradient of {param.grad.abs().mean().item()}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70c29c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate a model without skip connections\n",
    "#This has a vanishing gradient problem\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733cc000",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate a model with skip connections (shortcuts).\n",
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2df09b",
   "metadata": {},
   "source": [
    "## Chapter 4 Section 6 - Coding the GPT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a57080a",
   "metadata": {},
   "source": [
    "### 1.a Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5204d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead69b01",
   "metadata": {},
   "source": [
    "### 1.b Define the MultiHeadAttention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d242a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_in, d_out,\n",
    "        context_length, dropout,\n",
    "        num_heads, qkv_bias=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert (d_out%num_heads==0), \"d_out must be divisible by num_heads with no remainder.\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(\n",
    "                torch.ones(\n",
    "                        context_length,\n",
    "                        context_length\n",
    "                ),\n",
    "                diagonal = 1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #Initialize values and weight tensors\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        #Reshape and resize weight tensors to inflate them (opposite of flattning)\n",
    "        # Tensors start with shape (b,num_tokens, d_out) and end with a shape of\n",
    "        # (b, num_tokens, num_heads, head_dim)\n",
    "        # where head_dim = d_out/num_heads\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "\n",
    "        #Perform matrix multiplication to get attention scores:\n",
    "        # This is analagous to having a tensor, T with shape (1,2,3,4) and performing 2 dot products:\n",
    "        #  first = T[0,0,:,:]\n",
    "        #  first_result = first@first.T\n",
    "        #\n",
    "        # and then performing:\n",
    "        #\n",
    "        #  second = T[0,1,:,:]\n",
    "        # second_result = second@second.T\n",
    "        attn_scores = queries@keys.transpose(2,3)\n",
    "\n",
    "        #Mask attention scores\n",
    "        mask_bool = self.mask.bool()[:num_tokens,:num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        #Normalize attention scores\n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        #Dotproduct to get result\n",
    "        context_vec = (attn_weights@values).transpose(1,2)\n",
    "        \n",
    "        #Reshape outputs back to size (b,num_tokens,d_out)\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "            b, num_tokens, self.d_out\n",
    "        )\n",
    "\n",
    "        #Projection Layer - Not necessary, but often used in LLM architecture. I'm not sure why it's used.\n",
    "        # this layer projects the results from one vector space into another vector space, by some linear mapping.\n",
    "        # I'm not sure if this is to save disk space/memory, or if there is some optimal dimension for the vector\n",
    "        # space to decode this back to text. Maybe to decrease processing speed/cycles used.\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a823d4",
   "metadata": {},
   "source": [
    "### 1.c Define Feed Forward Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89268acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg['emb_dim'], cfg['emb_dim']),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961b9b46",
   "metadata": {},
   "source": [
    "## 1.d Define LayerNorm Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dcd8f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1,keepdim=True)\n",
    "        norm_x = (x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.scale*norm_x+self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d7d331",
   "metadata": {},
   "source": [
    "### 2. Define Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d434efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in = cfg['emb_dim'],\n",
    "            d_out = cfg['emb_dim'],\n",
    "            context_length = cfg['context_length'],\n",
    "            num_heads = cfg['n_heads'],\n",
    "            dropout = cfg['drop_rate'],\n",
    "            qkv_bias = cfg['qkv_bias'],\n",
    "        )\n",
    "        \n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = LayerNorm(cfg['emb_dim'])\n",
    "        self.drop_shortcut = nn.Dropout(cfg['drop_rate'])\n",
    "        \n",
    "    def forward(self,x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        \n",
    "        x = x + shortcut\n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        \n",
    "        x = x + shortcut\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2013dd",
   "metadata": {},
   "source": [
    "### 3. Define the GPTModel Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "217951db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Embedding(cfg['drop_rate'])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n",
    "        )\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg['emb_dim'],cfg['vocab_size'],bias=False\n",
    "        )\n",
    "        \n",
    "        def forward(self,in_idx):\n",
    "            batch_size, seq_len, = in_idx\n",
    "            tok_embeds = self.tok_emb(in_idx)\n",
    "            pos_embeds = self.pos_emb(\n",
    "                torch.arange(seq_len, device=in_idx.device)\n",
    "            )\n",
    "            \n",
    "            x = tok_embeds + pos_embeds\n",
    "            x = self.drop_emb(x)\n",
    "            x = self.trf_blocks(x)\n",
    "            x = self.final_norm(x)\n",
    "            \n",
    "            logits = self.out_head(x)\n",
    "            \n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b88e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "            'vocab_size':50257,\n",
    "            'context_length':1024,\n",
    "            'emb_dim':768,\n",
    "            'n_heads':12,\n",
    "            'n_layers':12,\n",
    "            'drop_rate':0.1,\n",
    "            'qkv_bias':False,\n",
    "        }\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def encoder(txt):\n",
    "    tensor = torch.tensor(tokenizer.encode(txt))\n",
    "    return tensor\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "txts = ['Every effort moves you','Every day holds a']\n",
    "batch = [encoder(tx) for tx in txts]\n",
    "batch = torch.stack(batch,dim=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
