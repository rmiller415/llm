{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b120826a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rwmil\\Documents\\GitHub\\llm\\llm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from itertools import chain\n",
    "\n",
    "fpath_win = 'C:\\\\Users\\\\rwmil\\\\Documents\\\\GitHub\\\\llm\\\\llm\\\\'\n",
    "fpath_mac = '/Users/richardmiller/Documents/GitHub/llm/'\n",
    "os.chdir(fpath_win)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ded437",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./LLMs-from-scratch-main/ch02/01_main-chapter-code/the-verdict.txt','r',encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "f.close()\n",
    "\n",
    "split_text = [re.split(r'([,.:;?_!\"()\\']|--|\\s)',i) for i in raw_text.split()]\n",
    "split_text = [i.strip() for item in split_text for i in item if i not in ['',' ']]\n",
    "split_text.extend(['<|endoftext|>','<|unk|>'])\n",
    "\n",
    "split_text = sorted(list(set(split_text)))\n",
    "vocab = {token:i for i,token in enumerate(split_text)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6f22941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self,vocab):\n",
    "        ''' The vocab object must be a dictionary or \n",
    "        dictionary-like object and must be defined \n",
    "        outside of the tokenizer class. It also must\n",
    "        have the form of {string:integer}'''\n",
    "        self.string_to_int = vocab\n",
    "        self.int_to_string = {i:s for s,i in vocab.items()}\n",
    "        self.re_string_encoder = r'([,.:;?_!\"()\\']|--|\\s)' #Leave these here because I can use them later to define the regex\n",
    "        self.re_string_decoder = r'\\s+([,.:;?!\"()\\'])' # strings that I want to use later. If my texts become more complex.\n",
    "\n",
    "    def encoder(self,text):\n",
    "        preprocess = re.split(self.re_string_encoder,text)\n",
    "        preprocess = [i.strip() for i in preprocess if i not in ['',' ']]\n",
    "        preprocess = [i if i in self.string_to_int else '<|unk|>' for i in preprocess]\n",
    "        \n",
    "        \n",
    "        return [self.string_to_int[s] for s in preprocess]\n",
    "\n",
    "    def decoder(self,ids):\n",
    "        text = ' '.join([self.int_to_string[i] for i in ids])\n",
    "        text = re.sub(self.re_string_decoder,r'\\1',text)\n",
    "        return text\n",
    "\n",
    "tokenizer = SimpleTokenizer(vocab)\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "te = \" <|endoftext|> \".join((text1, text2))\n",
    "out = tokenizer.encoder(te)\n",
    "out2 = tokenizer.decoder(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b9c15ca2-2e84-4919-82bd-3d97caec0a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "print(out2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
