{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b120826a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/richardmiller/Documents/GitHub/llm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from itertools import chain\n",
    "\n",
    "fpath_win = 'C:\\\\Users\\\\rwmil\\\\Documents\\\\GitHub\\\\llm\\\\llm\\\\'\n",
    "fpath_mac = '/Users/richardmiller/Documents/GitHub/llm/'\n",
    "os.chdir(fpath_mac)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ded437",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./LLMs-from-scratch-main/ch02/01_main-chapter-code/the-verdict.txt','r',encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "f.close()\n",
    "\n",
    "split_text = [re.split(r'([,.:;?_!\"()\\']|--|\\s)',i) for i in raw_text.split()]\n",
    "split_text = [i.strip() for item in split_text for i in item if i not in ['',' ']]\n",
    "split_text.extend(['<|endoftext|>','<|unk|>'])\n",
    "\n",
    "split_text = sorted(list(set(split_text)))\n",
    "vocab = {token:i for i,token in enumerate(split_text)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f22941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self,vocab):\n",
    "        ''' The vocab object must be a dictionary or \n",
    "        dictionary-like object and must be defined \n",
    "        outside of the tokenizer class. It also must\n",
    "        have the form of {string:integer}'''\n",
    "        self.string_to_int = vocab\n",
    "        self.int_to_string = {i:s for s,i in vocab.items()}\n",
    "        self.re_string_encoder = r'([,.:;?_!\"()\\']|--|\\s)' #Leave these here because I can use them later to define the regex\n",
    "        self.re_string_decoder = r'\\s+([,.:;?!\"()\\'])' # strings that I want to use later. If my texts become more complex.\n",
    "\n",
    "    def encoder(self,text):\n",
    "        preprocess = re.split(self.re_string_encoder,text)\n",
    "        preprocess = [i.strip() for i in preprocess if i not in ['',' ']]\n",
    "        preprocess = [i if i in self.string_to_int else '<|unk|>' for i in preprocess]\n",
    "        \n",
    "        \n",
    "        return [self.string_to_int[s] for s in preprocess]\n",
    "\n",
    "    def decoder(self,ids):\n",
    "        text = ' '.join([self.int_to_string[i] for i in ids])\n",
    "        text = re.sub(self.re_string_decoder,r'\\1',text)\n",
    "        return text\n",
    "\n",
    "tokenizer = SimpleTokenizer(vocab)\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "te = \" <|endoftext|> \".join((text1, text2))\n",
    "out = tokenizer.encoder(te)\n",
    "out2 = tokenizer.decoder(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c15ca2-2e84-4919-82bd-3d97caec0a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0062887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df41960",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "text = ('Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.')\n",
    "integers = tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d483273",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bcba90",
   "metadata": {},
   "source": [
    "## Section 2 Using Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7057da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "\n",
    "fpath_win = 'C:\\\\Users\\\\rwmil\\\\Documents\\\\GitHub\\\\llm\\\\llm\\\\'\n",
    "fpath_mac = '/Users/richardmiller/Documents/GitHub/llm/'\n",
    "os.chdir(fpath_mac)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58110c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get text data\n",
    "with open('./LLMs-from-scratch-main/ch02/01_main-chapter-code/the-verdict.txt') as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "f.close()\n",
    "\n",
    "#Initialize tokenizer and encode raw text from file\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "encoded_text = tokenizer.encode(raw_text)\n",
    "\n",
    "#Remove last 50 characters\n",
    "encoded_sample = encoded_text[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d105e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split encoded sample into x and y targets\n",
    "\n",
    "context_size = 4 #Determines number of tokens included in input\n",
    "x = encoded_sample[:context_size]\n",
    "y = encoded_sample[1:context_size+1]\n",
    "\n",
    "for i in range(1,context_size+1):\n",
    "    context = encoded_sample[:i]\n",
    "    desired = encoded_sample[i]\n",
    "    print(context,'----->',desired)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e0e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,context_size+1):\n",
    "    context = encoded_sample[:i]\n",
    "    desired = encoded_sample[i]\n",
    "    print(tokenizer.decode(context),'----->',tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097ea36",
   "metadata": {},
   "source": [
    "## Section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19ee2b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2bbbc7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self,txt,tokenizer,max_length,stride):\n",
    "        '''\n",
    "        Looks like tokenizer is instantiated before being passed to this class.\n",
    "        '''\n",
    "        \n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        #Get token id's\n",
    "        self.token_ids = tokenizer.encode(txt)\n",
    "        \n",
    "        for i in range(0,len(self.token_ids)-max_length,stride):\n",
    "            input_chunk = self.token_ids[i:i+max_length]\n",
    "            target_chunk = self.token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ids, self.target_ids\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6642d41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(\n",
    "    txt,\n",
    "    batch_size=25,\n",
    "    max_length=255,\n",
    "    stride=128,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "):\n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    dataset = GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    return dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58cec9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./LLMs-from-scratch-main/ch02/01_main-chapter-code/the-verdict.txt') as f:\n",
    "    raw_data = f.read()\n",
    "    \n",
    "f.close()\n",
    "\n",
    "#max_length and stride can be changed, using a larger stride will\n",
    "#result in less overfitting\n",
    "\n",
    "#batch_size is a tunable hyperparameter to make the model better.\n",
    "#Higher batch_size means more memory usage and less noise\n",
    "#Lower batch_size means less memory usage but more nosie in training\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_data,\n",
    "    batch_size=1,\n",
    "    max_length=4,\n",
    "    stride=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9184df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2,3,5,1])\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size,output_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ac03de88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256]) torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size,output_dim)\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_data,batch_size=8,max_length=max_length,stride=max_length,shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs,targets = next(data_iter)\n",
    "\n",
    "token_embeddings = token_embedding_layer(inputs[1])\n",
    "\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length,output_dim)\n",
    "pos_embedding = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embedding.shape,token_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ae94c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = token_embeddings+pos_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bc487e",
   "metadata": {},
   "source": [
    "# Chapter 3 - Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e12ed2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "inputs = torch.tensor([\n",
    "    [0.43,0.15,0.89],\n",
    "    [0.55,0.87,0.66],\n",
    "    [0.57,0.85,0.64],\n",
    "    [0.22,0.58,0.33],\n",
    "    [0.77,0.25,0.10],\n",
    "    [0.05,0.80,0.55],\n",
    "])\n",
    "\n",
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i,query)\n",
    "    \n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "134f101c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum:  tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2/attn_scores_2.sum()\n",
    "print('Attention Weights: ',attn_weights_2_tmp)\n",
    "print('Sum: ',attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2496cd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "#Computing attention weights of a single token by scalar multiplication of a vector\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "    \n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d6130248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "#Computing attention weights of all tokens by dot product\n",
    "attn_scores = torch.empty(6,6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i,j] = torch.dot(x_i,x_j)\n",
    "        \n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5f9cea47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs@inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c61a7b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores,dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec78f98",
   "metadata": {},
   "source": [
    "## Chatper 3 Section 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c712bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor([\n",
    "    [0.43,0.15,0.89],\n",
    "    [0.55,0.87,0.66],\n",
    "    [0.57,0.85,0.64],\n",
    "    [0.22,0.58,0.33],\n",
    "    [0.77,0.25,0.10],\n",
    "    [0.05,0.80,0.55],\n",
    "])\n",
    "\n",
    "#Define input element, input embadding size, and output embedding size\n",
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "\n",
    "#Initialize Weight tensors\n",
    "#requires_grad is set to false to reduce output clutter, but during model training it needs to be true\n",
    "torch.manual_seed(123) #For repeatability\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad=False)\n",
    "\n",
    "\n",
    "query_2 = x_2@W_query\n",
    "key_2 = x_2@W_key\n",
    "value_2 = x_2@W_value\n",
    "\n",
    "#Weight Tensors are optimized during training and are used to make predictions\n",
    "#Attention Weights are tensors that give the amount of attention to different parts of\n",
    "# the input. Attention weights are like saying since this word directly preceeds the unknown I\n",
    "# will pay more attention to it than I will to another word that is 40 tokens away.\n",
    "\n",
    "#Get all keys and values:\n",
    "keys = inputs@W_key\n",
    "values = inputs@W_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c26bd57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention score for position 2:  tensor(1.8524)\n",
      "Attention Scores:  tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n",
      "Scaled Attention Weights:  tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "Context Vector:  tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "#Computing an individual weight\n",
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print('Attention score for position 2: ',attn_score_22)\n",
    "\n",
    "#Compute all weights using tensor dot product\n",
    "attn_scores_2 = query_2@keys.T\n",
    "print('Attention Scores: ',attn_scores_2)\n",
    "\n",
    "#Notice that the second value in attn_scores_2 matches the value of attn_score_22\n",
    "\n",
    "#Calculate attention scores from attention weights\n",
    "#To help normalize we are dividing by the square root\n",
    "#I think this is going to be like the standard deviation where\n",
    "# you divide by sqrt(N) for a population.\n",
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2/d_k**0.5, dim=-1)\n",
    "print('Scaled Attention Weights: ',attn_weights_2)\n",
    "\n",
    "#Calculate context vector\n",
    "context_vect_2 = attn_weights_2 @ values\n",
    "print('Context Vector: ',context_vect_2)\n",
    "\n",
    "#Query refers to the current item (token, word, sentance, etc.)\n",
    "#Key is the indexing value of a token and is used to match the query\n",
    "#Value the actual content or input values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d2eda2",
   "metadata": {},
   "source": [
    "## Chapter 3 Section 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4c3a64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#It's useful to organize all the code in Chapter 3 Section 4.1 into a single\n",
    "# succinct python class for use in the LLM architecture.\n",
    "\n",
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self,d_in,d_out):\n",
    "        '''\n",
    "        Initializes weight tensors for queries, values, and keys.\n",
    "        Transforms from input dimension, d_in, to output dimension, d_out.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in,d_out))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        Compute keys, queries, and values tensors by cross product with their respective weights.\n",
    "        Calculate attention scores.\n",
    "        Normalize attention scores with softmax to get attention weight tensor.\n",
    "        Create a context vector.\n",
    "        \n",
    "        '''\n",
    "        keys = x@self.W_key\n",
    "        queries = x@self.W_query\n",
    "        values = x@self.W_value\n",
    "        \n",
    "        attn_scores = queries@keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores/keys.shape[-1]**0.5, \n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        context_vect = attn_weights@values\n",
    "        return context_vect\n",
    "    \n",
    "    \n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in,d_out)\n",
    "print(sa_v1(inputs))\n",
    "\n",
    "#NOTE: using nn.Linear layers will perform the matrix multiplication and with optimized weight initializations.\n",
    "# This should result in quicker and more reliable training versus using randomized weights.\n",
    "\n",
    "#Will impliment this in SelfAttention_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebe596c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries@keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores/keys.shape[-1]**0.5,\n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        context_vect = attn_weights@values\n",
    "        \n",
    "        return context_vect\n",
    "    \n",
    "    \n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in,d_out)\n",
    "\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efe3f10",
   "metadata": {},
   "source": [
    "# Chapter 3 Section 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23ea1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Casual Self-Attention (AKA Masked Attention): Prevents a model from learning words after the token word. \n",
    "#Because the model won't have access to information after some token. Restricts the model's ability to learn\n",
    "# from the previous data before the token.\n",
    "\n",
    "#Multi-Head Self-Attention: Splits the Self-Attention mechanism into multiple sub-units or 'heads', this\n",
    "# splits up the learning process. Each head learns from different parts of the input text or different contexts.\n",
    "# This allows or increases the model's ability to perform complex tasks.\n",
    "\n",
    "#Consider the case of data represented as a tensor, T. In 2-dimensions, the current token data is the diagonal\n",
    "# elements of the input array. So, the values above the diagonal are masked. In a multi-dimensional tensor, this\n",
    "# would correspond to the diagonal hyper-plane. All elements in the volume above the diagonal hyper-plane\n",
    "# will be masked. This prevents the model from accessing data after a given token and restricts it to data from\n",
    "# before the given token.\n",
    "\n",
    "#The unmasked weights are normalized to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ca2ae4",
   "metadata": {},
   "source": [
    "## Chapter 3 Section 5.1 - Applying a Casual Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86a66122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class from Chapter 3 Section 4.2\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries@keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores/keys.shape[-1]**0.5,\n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        context_vect = attn_weights@values\n",
    "        \n",
    "        return context_vect\n",
    "    \n",
    "    \n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in,d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75930eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights Tensor:  tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Simple Masking Tensor:  tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "Normalized Masking Tensor:  tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]])\n"
     ]
    }
   ],
   "source": [
    "#Compute attention weights with the SelfAttention_v2 Class from\n",
    "# Chapter 3 Section 4.2\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries@keys.T\n",
    "attn_weights = torch.softmax(\n",
    "    attn_scores/keys.shape[-1]**0.5,\n",
    "    dim=-1\n",
    ")\n",
    "\n",
    "#Print the tensor we want to mask:\n",
    "print('Attention Weights Tensor: ',attn_weights)\n",
    "\n",
    "#Use tril() function to create a masking tensor with 0's in the upper triangle\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "\n",
    "print('Simple Masking Tensor: ',mask_simple)\n",
    "\n",
    "#Normalize Masking Tensor\n",
    "row_sums = mask_simple.sum(dim=-1,keepdim=True)\n",
    "mask_simple_norm = mask_simple/row_sums\n",
    "print('Normalized Masking Tensor: ', mask_simple_norm)\n",
    "\n",
    "#Masking and renormalizing will prevent upper triangular data from the softmax from leaking into\n",
    "# the masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "476702f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking Tensor:  tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "Attention Weights:  tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Context Tensor:  tensor([[0.1855, 0.8812],\n",
      "        [0.2795, 0.9361],\n",
      "        [0.3133, 0.9508],\n",
      "        [0.2994, 0.8595],\n",
      "        [0.2702, 0.7554],\n",
      "        [0.2772, 0.7618]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Improving on the masking function\n",
    "\n",
    "#Softmax converts the data into a Poisson probability distribution. We can represent zeros in the upper\n",
    "# triangle matrix as e^-inf. This can be beneficial to us. If we replace the upper triangle of the tensor with -inf\n",
    "# and then take the softmax, it will become an upper triangle of masked zeros.\n",
    "\n",
    "#Define an improved masking tensor:\n",
    "mask = torch.triu(torch.ones(context_length,context_length),diagonal=1) #diagonal=1 starts at the line above the diagonal\n",
    "masked = attn_scores.masked_fill(mask.bool(),-torch.inf) #Uses the masked_fill method in pytorch to fill the upper triangle with -infinity\n",
    "\n",
    "print('Masking Tensor: ',masked)\n",
    "\n",
    "#Make Attention Weights tensor\n",
    "attn_weights = torch.softmax(masked/keys.shape[-1]**0.5, dim=1)\n",
    "print('Attention Weights: ',attn_weights)\n",
    "\n",
    "#Calculate Context Tensor:\n",
    "context_vect = attn_weights@values\n",
    "print('Context Tensor: ', context_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a391b6c0",
   "metadata": {},
   "source": [
    "## Chapter 3 Section 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b06c1ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "#Dropout can be used to help prevent overfitting to available data and increase general usefulness of\n",
    "# your model. It does this by ignoring hidden layer units.\n",
    "\n",
    "#Dropout is only used during training and is disabled for model predictions.\n",
    "\n",
    "#For transformer architectures, dropout is typically employed at two points in the training process.\n",
    "# 1. After calculating attention weights\n",
    "# 2. After applying attention weights to value vectors\n",
    "\n",
    "#Here we will use a dropout rate of 50%, but typically dropout rates are 10% to 20% (0.1 to 0.2)\n",
    "\n",
    "#Apply the dropout to a 6x6 tensor for illustration purposes:\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6,6)\n",
    "print(dropout(example))\n",
    "\n",
    "#To account for half of the values being dropped out, the other values are doubled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6214c224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Apply the dropout method to the attention weights:\n",
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d564251",
   "metadata": {},
   "source": [
    "## Chapter 3 Section 5.3 - Implementing a Compact Casual Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b70c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impliment a CasualAttention class by modifying the SelfAttention class. This will then be used\n",
    "# later as a template for the MultiHeadAttion class that we will code.\n",
    "\n",
    "class CasualAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in,\n",
    "        d_out,\n",
    "        context_length,\n",
    "        dropout,\n",
    "        qkv_bias = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length,context_length), diagonal=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        b,num_tokens,d_in = x.shape\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries@keys.transpose(1,2)\n",
    "        attn_scores.masked_fill(\n",
    "            self.mask.bool()[:num_tokens,:num_tokens],\n",
    "            -torch.inf\n",
    "        )\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores/keys.shape[-1]**0.5,\n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vect = attn_weights@values\n",
    "        \n",
    "        return context_vect\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c706ee8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (6) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m context_length \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      6\u001b[0m ca \u001b[38;5;241m=\u001b[39m CasualAttention(d_in, d_out, context_length, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m context_vect \u001b[38;5;241m=\u001b[39m \u001b[43mca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContext Tensor: \u001b[39m\u001b[38;5;124m'\u001b[39m,context_vect)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[44], line 32\u001b[0m, in \u001b[0;36mCasualAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_value(x)\n\u001b[1;32m     31\u001b[0m attn_scores \u001b[38;5;241m=\u001b[39m queries\u001b[38;5;129m@keys\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mattn_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnum_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnum_tokens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(\n\u001b[1;32m     37\u001b[0m     attn_scores\u001b[38;5;241m/\u001b[39mkeys\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m     38\u001b[0m     dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     41\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_weights)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (6) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "#Make a batched set of identical data inputs:\n",
    "batch = torch.stack((inputs,inputs), dim=0)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[-1]\n",
    "ca = CasualAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vect = ca(batch)\n",
    "print('Context Tensor: ',context_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42988d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
